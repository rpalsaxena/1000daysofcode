{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rpals\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\rpals\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\rpals\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\rpals\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\rpals\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\rpals\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\rpals\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\rpals\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\rpals\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\rpals\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "    \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rpals\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.95s)\n",
      "creating index...\n",
      "index created!\n",
      "[50000/414113] Tokenized the captions.\n",
      "[100000/414113] Tokenized the captions.\n",
      "[150000/414113] Tokenized the captions.\n",
      "[200000/414113] Tokenized the captions.\n",
      "[250000/414113] Tokenized the captions.\n",
      "[300000/414113] Tokenized the captions.\n",
      "[350000/414113] Tokenized the captions.\n",
      "[400000/414113] Tokenized the captions.\n",
      "Total vocabulary size: 9956\n",
      "Saved the vocabulary wrapper to './coco2014/vocabulary.pkl'\n"
     ]
    }
   ],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        self.index = 0\n",
    " \n",
    "    def __call__(self, token):\n",
    "        if not token in self.w2i:\n",
    "            return self.w2i['<unk>']\n",
    "        return self.w2i[token]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.w2i)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if not token in self.w2i:\n",
    "            self.w2i[token] = self.index\n",
    "            self.i2w[self.index] = token\n",
    "            self.index += 1\n",
    "            \n",
    "def build_vocabulary(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    " \n",
    "        if (i+1) % 50000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    " \n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    tokens = [token for token, cnt in counter.items() if cnt >= threshold]\n",
    " \n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocab()\n",
    "    vocab.add_token('<pad>')\n",
    "    vocab.add_token('<start>')\n",
    "    vocab.add_token('<end>')\n",
    "    vocab.add_token('<unk>')\n",
    " \n",
    "    # Add the words to the vocabulary.\n",
    "    for i, token in enumerate(tokens):\n",
    "        vocab.add_token(token)\n",
    "    return vocab\n",
    " \n",
    "vocab = build_vocabulary(json='coco2014/annotations/captions_train2014.json', threshold=4)\n",
    "vocab_path = './coco2014/vocabulary.pkl'\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9956"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[1000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[1100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[1200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[1300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[1400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[1500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[1600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[1700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[1800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[1900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[2000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[2100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[2200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[2300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[2400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[2500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[2600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[2700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[2800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[2900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[3000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[3100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[3200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[3300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[3400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[3500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[3600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[3700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[3800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[3900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[4000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[4100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[4200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[4300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[4400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[4500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[4600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[4700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[4800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[4900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[5000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[5100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[5200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[5300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[5400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[5500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[5600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[5700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[5800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[5900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[6000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[6100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[6200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[6300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[6400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[6500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[6600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[6700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[6800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[6900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[7000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[7100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[7200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[7300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[7400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[7500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[7600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[7700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[7800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[7900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[8000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[8100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[8200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[8300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[8400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[8500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[8600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[8700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[8800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[8900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[9000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[9100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[9200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[9300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[9400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[9500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[9600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[9700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[9800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[9900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[10000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[10100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[10200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[10300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[10400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[10500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[10600/82783] Resized the images and saved into './coco2014/resized_images/'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[10800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[10900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[11000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[11100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[11200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[11300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[11400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[11500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[11600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[11700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[11800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[11900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[12000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[12100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[12200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[12300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[12400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[12500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[12600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[12700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[12800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[12900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[13000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[13100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[13200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[13300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[13400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[13500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[13600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[13700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[13800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[13900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[14000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[14100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[14200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[14300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[14400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[14500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[14600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[14700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[14800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[14900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[15000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[15100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[15200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[15300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[15400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[15500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[15600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[15700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[15800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[15900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[16000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[16100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[16200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[16300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[16400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[16500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[16600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[16700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[16800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[16900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[17000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[17100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[17200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[17300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[17400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[17500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[17600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[17700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[17800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[17900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[18000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[18100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[18200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[18300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[18400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[18500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[18600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[18700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[18800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[18900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[19000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[19100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[19200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[19300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[19400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[19500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[19600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[19700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[19800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[19900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[20000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[20100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[20200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[20300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[20400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[20500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[20600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[20700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[20800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[20900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[21000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[21100/82783] Resized the images and saved into './coco2014/resized_images/'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[21300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[21400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[21500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[21600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[21700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[21800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[21900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[22000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[22100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[22200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[22300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[22400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[22500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[22600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[22700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[22800/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[22900/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[23000/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[23100/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[23200/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[23300/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[23400/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[23500/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[23600/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[23700/82783] Resized the images and saved into './coco2014/resized_images/'.\n",
      "[23800/82783] Resized the images and saved into './coco2014/resized_images/'.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "image file is truncated (9 bytes not processed)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0aa49a813bcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./coco2014/resized_images/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mimage_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mreshape_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-0aa49a813bcc>\u001b[0m in \u001b[0;36mreshape_images\u001b[1;34m(image_path, output_path, shape)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r+b'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreshape_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-0aa49a813bcc>\u001b[0m in \u001b[0;36mreshape_image\u001b[1;34m(image, shape)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreshape_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Resize an image to the given shape.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreshape_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   1914\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1916\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m                                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m                                     raise OSError(\n\u001b[1;32m--> 260\u001b[1;33m                                         \u001b[1;34m\"image file is truncated \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m                                         \u001b[1;34mf\"({len(b)} bytes not processed)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                                     )\n",
      "\u001b[1;31mOSError\u001b[0m: image file is truncated (9 bytes not processed)"
     ]
    }
   ],
   "source": [
    "def reshape_image(image, shape):\n",
    "    \"\"\"Resize an image to the given shape.\"\"\"\n",
    "    return image.resize(shape, Image.ANTIALIAS)\n",
    " \n",
    "def reshape_images(image_path, output_path, shape):\n",
    "    \"\"\"Reshape the images in 'image_path' and save into 'output_path'.\"\"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    " \n",
    "    images = os.listdir(image_path)\n",
    "    num_im = len(images)\n",
    "    for i, im in enumerate(images):\n",
    "        with open(os.path.join(image_path, im), 'r+b') as f:\n",
    "            with Image.open(f) as image:\n",
    "                image = reshape_image(image, shape)\n",
    "                image.save(os.path.join(output_path, im), image.format)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
    "                   .format(i+1, num_im, output_path))\n",
    "\n",
    "image_path = './coco2014/train2014/'\n",
    "output_path = './coco2014/resized_images/'\n",
    "image_shape = [256, 256]\n",
    "reshape_images(image_path, output_path, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, data_path, coco_json_path, vocabulary, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = data_path\n",
    "        self.coco_data = COCO(coco_json_path)\n",
    "        self.indices = list(self.coco_data.anns.keys())\n",
    "        self.vocabulary = vocabulary\n",
    "        self.transform = transform\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        coco_data = self.coco_data\n",
    "        vocabulary = self.vocabulary\n",
    "        annotation_id = self.indices[idx]\n",
    "        caption = coco_data.anns[annotation_id]['caption']\n",
    "        image_id = coco_data.anns[annotation_id]['image_id']\n",
    "        image_path = coco_data.loadImgs(image_id)[0]['file_name']\n",
    " \n",
    "        image = Image.open(os.path.join(self.root, image_path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    " \n",
    "        # Convert caption (string) to word ids.\n",
    "        word_tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocabulary('<start>'))\n",
    "        caption.extend([vocabulary(token) for token in word_tokens])\n",
    "        caption.append(vocabulary('<end>'))\n",
    "        ground_truth = torch.Tensor(caption)\n",
    "        return image, ground_truth\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    " \n",
    " \n",
    "def collate_function(data_batch):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data_batch.sort(key=lambda d: len(d[1]), reverse=True)\n",
    "    imgs, caps = zip(*data_batch)\n",
    " \n",
    "    # Merge images (from list of 3D tensors to 4D tensor).\n",
    "    # Originally, imgs is a list of <batch_size> number of RGB images with dimensions (3, 256, 256)\n",
    "    # This line of code turns it into a single tensor of dimensions (<batch_size>, 3, 256, 256)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    " \n",
    "    # Merge captions (from list of 1D tensors to 2D tensor), similar to merging of images donw above.\n",
    "    cap_lens = [len(cap) for cap in caps]\n",
    "    tgts = torch.zeros(len(caps), max(cap_lens)).long()\n",
    "    for i, cap in enumerate(caps):\n",
    "        end = cap_lens[i]\n",
    "        tgts[i, :end] = cap[:end]        \n",
    "    return imgs, tgts, cap_lens\n",
    " \n",
    "def get_loader(data_path, coco_json_path, vocabulary, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco_dataser = CustomCocoDataset(data_path=data_path,\n",
    "                       coco_json_path=coco_json_path,\n",
    "                       vocabulary=vocabulary,\n",
    "                       transform=transform)\n",
    "    \n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    custom_data_loader = torch.utils.data.DataLoader(dataset=coco_dataser, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_function)\n",
    "    return custom_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        module_list = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet_module = nn.Sequential(*module_list)\n",
    "        self.linear_layer = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, input_images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            resnet_features = self.resnet_module(input_images)\n",
    "        resnet_features = resnet_features.reshape(resnet_features.size(0), -1)\n",
    "        final_features = self.batch_norm(self.linear_layer(resnet_features))\n",
    "        return final_features\n",
    " \n",
    " \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def forward(self, input_features, capts, lens):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embedding_layer(caps)\n",
    "        embeddings = torch.cat((input_features.unsqueeze(1), embeddings), 1)\n",
    "        lstm_input = pack_padded_sequence(embeddings, lens, batch_first=True) \n",
    "        hidden_variables, _ = self.lstm_layer(lstm_input)\n",
    "        model_outputs = self.linear_layer(hidden_variables[0])\n",
    "        return model_outputs\n",
    "    \n",
    "    def sample(self, input_features, lstm_states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_indices = []\n",
    "        lstm_inputs = input_features.unsqueeze(1)\n",
    "        for i in range(self.max_seq_len):\n",
    "            hidden_variables, lstm_states = self.lstm_layer(lstm_inputs, lstm_states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            model_outputs = self.linear_layer(hidden_variables.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted_outputs = model_outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_indices.append(predicted_outputs)\n",
    "            lstm_inputs = self.embedding_layer(predicted_outputs)                       # inputs: (batch_size, embed_size)\n",
    "            lstm_inputs = lstm_inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_indices = torch.stack(sampled_indices, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.90s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to C:\\Users\\rpals/.cache\\torch\\checkpoints\\resnet152-b121ed2d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2c93a6225d4631ace93cbef04cc2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=241530880), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-154bce108b68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mtotal_num_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Set mini-batch dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    717\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Create model directory\n",
    "if not os.path.exists('models_dir/'):\n",
    "    os.makedirs('models_dir/')\n",
    "\n",
    "    \n",
    "# Image preprocessing, normalization for the pretrained resnet\n",
    "transform = transforms.Compose([ \n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "\n",
    "# Load vocabulary wrapper\n",
    "with open('coco2014/vocabulary.pkl', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "    \n",
    "# Build data loader\n",
    "custom_data_loader = get_loader('coco2014/train2014', 'coco2014/annotations/captions_train2014.json', vocabulary, \n",
    "                         transform, 128,\n",
    "                         shuffle=True, num_workers=2) \n",
    "\n",
    "\n",
    "# Build the models\n",
    "encoder_model = CNNModel(256).to(device)\n",
    "decoder_model = LSTMModel(256, 512, len(vocabulary), 1).to(device)\n",
    " \n",
    "    \n",
    "# Loss and optimizer\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "parameters = list(decoder_model.parameters()) + list(encoder_model.linear_layer.parameters()) + list(encoder_model.batch_norm.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
    "\n",
    "\n",
    "# Train the models\n",
    "total_num_steps = len(custom_data_loader)\n",
    "for epoch in range(5):\n",
    "    for i, (imgs, caps, lens) in enumerate(custom_data_loader):\n",
    " \n",
    "        # Set mini-batch dataset\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        tgts = pack_padded_sequence(caps, lens, batch_first=True)[0]\n",
    " \n",
    "        # Forward, backward and optimize\n",
    "        feats = encoder_model(imgs)\n",
    "        outputs = decoder_model(feats, caps, lens)\n",
    "        loss = loss_criterion(outputs, tgts)\n",
    "        decoder_model.zero_grad()\n",
    "        encoder_model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # Print log info\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, 5, i, total_num_steps, loss.item(), np.exp(loss.item()))) \n",
    " \n",
    "        # Save the model checkpoints\n",
    "        if (i+1) % 1000 == 0:\n",
    "            torch.save(decoder_model.state_dict(), os.path.join(\n",
    "                'models_dir/', 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "            torch.save(encoder_model.state_dict(), os.path.join(\n",
    "                'models_dir/', 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_path = 'sample.jpg'\n",
    " \n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "\n",
    "def load_image(image_file_path, transform=None):\n",
    "    img = Image.open(image_file_path).convert('RGB')\n",
    "    img = img.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        img = transform(img).unsqueeze(0)\n",
    "    \n",
    "    return img\n",
    " \n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "\n",
    "# Load vocabulary wrapper\n",
    "with open('coco2014/vocabulary.pkl', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "\n",
    "# Build models\n",
    "encoder_model = CNNModel(256).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "decoder_model = LSTMModel(256, 512, len(vocabulary), 1)\n",
    "encoder_model = encoder_model.to(device)\n",
    "decoder_model = decoder_model.to(device)\n",
    "\n",
    "\n",
    "# Load the trained model parameters\n",
    "encoder_model.load_state_dict(torch.load('models_dir/encoder-2-3000.ckpt'))\n",
    "decoder_model.load_state_dict(torch.load('models_dir/decoder-2-3000.ckpt'))\n",
    "\n",
    "\n",
    "# Prepare an image\n",
    "img = load_image(image_file_path, transform)\n",
    "img_tensor = img.to(device)\n",
    "\n",
    "\n",
    "# Generate an caption from the image\n",
    "feat = encoder_model(img_tensor)\n",
    "sampled_indices = decoder_model.sample(feat)\n",
    "sampled_indices = sampled_indices[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
    "\n",
    "\n",
    "# Convert word_ids to words\n",
    "predicted_caption = []\n",
    "for token_index in sampled_indices:\n",
    "    word = vocabulary.i2w[token_index]\n",
    "    predicted_caption.append(word)\n",
    "    if word == '<end>':\n",
    "        break\n",
    "predicted_sentence = ' '.join(predicted_caption)\n",
    "\n",
    "\n",
    "# Print out the image and the generated caption\n",
    "print (predicted_sentence)\n",
    "img = Image.open(image_file_path)\n",
    "plt.imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
